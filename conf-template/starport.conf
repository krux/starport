krux.starport {

    # the prefix to be used for pipeline names that are controled by starport
    prefix = "sp_"

    # the starport jar to be deployed replace this with valid s3 uri
    jar.url = ??? # "s3://some-fake-bucket/starport-core-assembly-current.jar"

    # sns notifications to be sent when starport fails
    notification.sns = ??? # "arn:aws:sns:<region>:<account-id>:<topic-name>"

    jdbc {
        slick {
            dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
            connectionPool = disabled
            properties = {
                databaseName = "starport"  # the database name to use
                serverName = ???  # the database host name
                user = ???  # the database username
                password = ??? # the password
            }
        }
    }

    # number of tasks to use per core, set 0 to use the default
    parallel = 3

    # maximum number of pipelines to schedule per run
    max_pipelines = 60

    # Environment variables to be added to the runner
    extra_envs {
        EXAMPLE_ENVIRONMENT_VAR_1 = "some-value"
        EXAMPLE_ENVIRONMENT_VAR_2 = "some-value"
    }

    # slack webhook to be used (optional)
    slack_webhook_url = ???

    # graphite integration (optional)
    metric.graphite {
        # the graphite hosts
        hosts = [
            "host1.nonexist-graphite-host.net",
            "host2.nonexist-graphite-host.net"
        ]

        # the graphite port number
        port = 9518

        # the metric prefix to be used
        prefix = "stats.prod.starport"
    }
}

# Starport rely hyperion to run, here are some sample hyperion configurations
hyperion {

    script.uri = "s3://krux-datapipeline/scripts/"

    aws {

        client.max_retry = 10

        # the ec2 instance configuration to be used to run starport processes
        ec2 {
            instance.type = "m3.medium"
            securitygroup = ???  # the security group
            terminate = "1 hours"  # try not to change this one
            image {
                # Make sure it's AWS data pipeline compatible image and the AMI
                # is configured to use Java 8 by default
                us-east-1 = ???  # the AMI
            }
        }
    }
}
