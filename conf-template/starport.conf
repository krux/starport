krux.starport {

    # (Optional) make sure all pipelines are scheduled in the configured region
    region = null

    # the prefix to be used for pipeline names that are controled by starport
    prefix = "sp_"

    # the starport jar to be deployed replace this with valid s3 uri
    jar.url = "???" # "s3://some-fake-bucket/starport-core-assembly-current.jar"

    notification {
        # sns notifications to be sent when starport fails
        sns = "???" # "arn:aws:sns:<region>:<account-id>:<topic-name>"
        email {
            from = "???" # no-reply@myco.com
            to = ["???"] # [ "tellme@myco.com", "tellme2@myco.com" ]
        }
    }
    jdbc {
        slick {
            dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
            connectionPool = disabled
            properties = {
                databaseName = "starport"  # the database name to use
                serverName = "???"  # the database host name
                user = "???"  # the database username
                password = "???" # the password
            }
        }
    }

    # number of tasks to use per core, set 0 to use the default
    parallel = 3

    # maximum number of pipelines to schedule per run
    max_pipelines = 60

    # Environment variables to be added to the runner
    extra_envs {
        EXAMPLE_ENVIRONMENT_VAR_1 = "some-value"
        EXAMPLE_ENVIRONMENT_VAR_2 = "some-value"
    }

    # remote scheduler
    remote.scheduler {
        is_enabled = false
        sqs_return_address = "" #optional if is_enabled=false
        sqs_to_address = "" #optional if is_enabled=false
        aws_endpoint_url = "" #optional
    }

    dispatcher {
      # valid values are default or sqs
      type = "default"
      # applicable if type is set to sqs
      sqs {
        publishQueueUrl = ""
        retrieveQueueUrl = ""
      }
    }

    # slack webhook to be used (optional)
    slack_webhook_url = "???"

    # metrics engine (optional)
    # Valid values: null, "graphite", "cloudwatch"
    # Note that only one option can be specified
    metric.engine = null

    # graphite integration (optional)
    metric.graphite {
        # the graphite hosts
        hosts = [
            "host1.nonexist-graphite-host.net",
            "host2.nonexist-graphite-host.net"
        ]

        # the graphite port number
        port = 9518

        # the metric prefix to be used
        prefix = "stats.prod.starport"
    }

    # cloudwatch integration (optional)
    metric.cloudwatch {

      # Specify the region to report to.
      region = null

      # An optional list of dimensions to report to Cloudwatch for filtering.
      "Dimensions" : {

          "Environment" : "???"
          "Stack" : "???"
          "Name" : "???"

      }

    }

}

# Starport rely hyperion to run, here are some sample hyperion configurations
hyperion {

    script.uri = "s3://krux-datapipeline/scripts/"

    aws {

        client.max_retry = 10

        # the ec2 instance configuration to be used to run starport processes
        ec2 {
            instance.type = "m3.medium"
            securitygroup = "???"  # the security group
            terminate = "1 hours"  # try not to change this one
            image {
                # Make sure it's AWS data pipeline compatible image and the AMI
                # is configured to use Java 8 by default
                us-east-1 = "???"  # the AMI
            }
        }
    }
}
